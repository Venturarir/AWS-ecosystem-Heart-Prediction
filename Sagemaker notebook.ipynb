{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling al the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the dataset we have saved in our s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    's3://group3-finalproject/Medicaldataset.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exact dataset, we don't need to perform any cleaning process. Even so, we left a cleaning pipeline to perform automatically in anyfuture dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Numeric features pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  \n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "# Categorical features pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['num_feature1', 'num_feature2']),\n",
    "        ('cat', categorical_transformer, ['cat_feature1', 'cat_feature2'])\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)]) #Use the pipeline to fit transform the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, testval = train_test_split(data, train_size=0.8, random_state=1200)\n",
    "val, test = train_test_split(testval, train_size=0.5, random_state=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a python function to upload the validation and train data to the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def upload_to_s3(df, bucket, filename):\n",
    "    \n",
    "    placeholder = io.StringIO()\n",
    "    df.to_csv(placeholder, header=False, index=False)\n",
    "    object = s3.Object(bucket, filename)\n",
    "    object.put(Body=placeholder.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(train, 'group3-finalproject', 'train.csv')\n",
    "upload_to_s3(val, 'group3-finalproject', 'val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the Amazon ECR URI for a specific version of the XGBoost algorithm Docker image in the AWS region \"eu-west-3\" using the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = sagemaker.image_uris.retrieve('xgboost', 'eu-west-3', version='0.90-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an Amazon SageMaker estimator for an XGBoost model and sets up the model's output to be stored in a specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region_name = boto3.Session().region_name\n",
    "#container = get_image_uri(region_name, 'xgboost', '0.90-1')  # Old version. Works anyway but warns.  \n",
    "container = sagemaker.image_uris.retrieve('xgboost', region_name, version='0.90-1')\n",
    "output_location = 's3://group3-finalproject/'\n",
    "\n",
    "#For a list of possible parameters of xgboost, see\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters\n",
    "hyperparams = {\n",
    "    'num_round': '20',\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    output_path=output_location,\n",
    "    hyperparameters=hyperparams,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to create the train and validation datasets to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    's3://group3-finalproject/train.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "val_channel = sagemaker.session.s3_input(\n",
    "    's3://group3-finalproject/val.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "\n",
    "channels_for_training = {\n",
    "    'train': train_channel,\n",
    "    'validation': val_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs=channels_for_training, logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator._current_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function analytics to get the metrics results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = sagemaker.analytics.TrainingJobAnalytics(\n",
    "    estimator._current_job_name,\n",
    "    metric_names=['train:rmse', 'validation:rmse']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.dataframe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
